---
title: "Music Psychology Data challenge"
format: gfm
---

![](figures/MP_data_challenge_smaller.png){width=200 fig-align="left"}

# Music Psychology Data Challenge 1

This is an internal training exercise for the _Durham Music and Science_ _Music Psychology Lab_, where all members—working individually or in small groups—are invited to develop models that explain data in a reliable and robust way. We assume that most participants have a basic understanding of statistics and are familiar with constructing regression models, which serves as the foundation for this task.

To make the exercise more challenging and promote the acquiring better modeling principles, I have built in some inherent challenges to the task: 

1. We have an abundance of data, particularly predictors
2. We lack a guiding theory to form our assumptions
3. We want the analyses to be transparent (notebooks or sets of codes that can run independently)

It would be also desirable to be able to explain what the model does in plain language.

## Explain emotion ratings using acoustic descriptors

For this task, let's focus on static ratings (a single aggregated mean rating for the whole excerpt) using static musical features already extracted from music using MIR tools. We have a good dataset for this.

### Dataset: PMEmo – A Dataset For Music Emotion Computing

Dataset is available at [https://github.com/HuiZhangDB/PMEmo](https://github.com/HuiZhangDB/PMEmo) and fully at [http://huisblog.cn/PMEmo/](http://huisblog.cn/PMEmo/) and this is the description by the authors:

> PMEmo dataset contains emotion annotations of 794 songs as well as the simultaneous electrodermal activity (EDA) signals. A Music Emotion Experiment was well-designed for collecting the affective-annotated music corpus of high quality, which recruited 457 subjects.

> The dataset is publically available to the research community, which is foremost intended for benchmarking in music emotion retrieval and recognition. To straightforwardly evaluate the methodologies for music affective analysis, it also involves pre-computed audio feature sets. In addition to that, manually selected chorus excerpts (compressed in MP3) of songs are provided to facilitate the development of chorus-related research.

For more details, see paper by [Zhang et al., (2018)](https://doi.org/10.1145/3206025.3206037).

The data allows to explore various aspects of emotion induction, including (a) predicting arousal and valence from acoustic features, (b) predicting participants' electrodermal activity (EDA) from continuous musical features, and (c) to explore the impact of lyrics or other data on either of these. Let's start with the easiest task, predicting the rated emotions with acoustic features.

#### Loading data

To facilitate analysis and to avoid everyone downloading the full dataset (1.3 Gb), I share the minimal data in GitHub (just copy this repository).

```{r}
#| id: get-data
# get static annotations
anno <- read.csv('data/static_annotations.csv',header = TRUE)
# get static acoustic features
feat <- read.csv('data/static_features.csv',header = TRUE)
# combine ratings and features (leave metadata out for now)
df <- merge(anno,feat,by="musicId")
knitr::kable(head(df[,1:7]))
```
In the dataframe `df` we have everything we need for the task, where the first column contains the `musicId` and the columns 2 and 3 are the arousal and valence mean (ratings) and the rest of the columns are individual acoustic features. Note the size of the dataset, `r nrow(df)` rows representing excerpts with `r dim(df)[2]` columns representing variables (`musicId`,`Arousal.mean.`, `Valence.mean.` and `r dim(df)[2]-3` more columns with exotic names related to audio features).

#### Meta-data (not used in this task)

It might be useful to see the metadata in the `meta` dataframe. If you wish to get the audio examples, this data is useful to link up the audio and gives the track names and artist and album names. For a visualisation, you might want join the two dataframes (`df` and `meta`), but for the sake of the task, working with `df` is sufficient.

```{r}
#| id: metadata
# get metadata
meta <- read.csv('data/metadata.csv',header = TRUE)
knitr::kable(head(meta))
```

### Building a bad model

The simplest model would be a linear regression predicting either arousal or valence using all features, and it could be done in R using (`lm`) in the following way:

```{r}
#| eval: true
#| id: bad-regression
bad_model <- lm(Arousal.mean. ~ ., 
  data = dplyr::select(df,-musicId,-Valence.mean.)) # discard unwanted columns
s <- summary(bad_model)
print(round(s$r.squared,3))
```

This bad model is *pure nonsense* because it tries to predict `r nrow(df)` ratings with `r dim(df)[2]-3` predictors, and if you have more predictors than observations, you break any modelling assumptions and you will be explain all data even with random variables. As we can see, the model is "perfect" ($r^2$ = `r round(s$r.squared,3)`). To predict a variable, you need many more observations than predictors and the rule is to have 15 or 20 times more observations than predictors. 

Just to demonstrate this, here we predict arousal with 770 random features, which is equally "good" as the previous bad model.

```{r}
#| eval: true
#| id: random-regression
n <- nrow(df); reps <- 770; n1 <- 1
# create a data frame with 770 random variables 
tmp<- as.data.frame(cbind(matrix(seq_len(n*n1), ncol=n1),
      matrix(sample(0:1, n*reps, replace=TRUE), ncol=reps)))
tmp$Arousal.mean. <- df$Arousal.mean. # add arousal to random data
random_model <- lm(Arousal.mean. ~ .,data=tmp)
s2 <- summary(random_model)
print(round(s2$r.squared,3))
```
A perfect prediction obtained with noise! This illustrates that we need to be selective about the predictors as we can only afford to have a maximum of 30 predictors if using the rule of 20:1, but consider having only a handful predictors, if you want to be able to explain the model. 

### Building a good model

To do the modelling properly, consider some of the following steps:

1. Divide the data into training and testing subsets (typically 80%/20%) to avoid overfitting.

3. When building the model, consider normalising your predictors (the predictors contain widely different magnitudes, which could be a problem for interpretation and developing the models).

4. When building the model with a training subset, consider cross-validation (to avoid overfitting).

5. Have some kind of feature selection principle (theory, statistical operation, intuition, ...).

6. Try to create as simple a model as possible.

7. Assess the goodness of the model with separate data (the testing subset typically serves this purpose).

8. Explain what explains arousal and valence based on your analysis.

There are plenty of guides about how to create models in R and in Python, many of them using useful packages designed for building models such [`caret` package](https://topepo.github.io/caret/index.html) or [`tidymodels`](https://www.tidymodels.org) or other guides such as [YaRrr!](https://bookdown.org/ndphillips/YaRrr/). Classic statistics handbooks will give you solid guidance as well (Howell, 2010; Tabachnick et al., 2013), some of which come with R code (Lilja and Linse, 2016; Sheather, 2009; McNulty, 2021).

## To submit your model

You can do the challenge individually or in groups. When you have done the exercise, you should be able:

1. to show the analytical steps 
2. to summarise your best model using metrics such as $R^2$
3. to explain the model by interpreting the model coefficients
4. to tell others what did you learn while doing the exercise (note: fails and dead ends are important part of the learning process)

For the 3rd point, many of the acoustical descriptors might be quite difficult to interpret from the labels alone, but you can still explain the model principles even if the computation or the meaning of the exact feature may not be easily decipher.

Bonus points for visualising the original ratings and the model predictions.

We want the outcome to be shared as a notebook/document (_Rmarkdown_, _Quarto_, _Jupyter_ or even pure R and Python) that will be able to run and produce your analysis in any computer (with the same data and packages). This part of the exercise encourages you to build transparent models that others will understand and can run. 

If you enjoy the task, we could later on try a more challenging variant related to this where we attempt to explain the electodermal activity with musical features or bring information from lyrics or metadata to the models.

![](figures/MPL.png){width=160 fig-align="left"}
### Postscriptum: Winner of the Challenge 1

**Wei Wu** developed a mature modelling approach for this challenge, where he took those features that correlated with the ratings and reduced the number of features through principal component analysis (PCA). He then used the PCA components to predict the ratings with proper cross-validation. All submission managed to do meaningful analyses of the data. One of the takeaways was that the model development and evaluation needs to be kept separate from the start to avoid "peeking". 

# Music Psychology Data Challenge 2

The second music psychology data challenge is about noise and errors. How do you diagnose and deal with these?

## Priming data

This dataset is from a priming study, where the participants have done quick binary judgements of negative and positive words (as either "negative" or "positive"). The words have been presented just after a sound stimulus (called "prime" in priming studies), which has been either positive or negative (say consonant or dissonant). A long-known fact is that processing incongruous stimuli takes more time than congruous, so the reaction time gives a window into this process, which is not under volitional control. It allows researchers to explore whether sound association are truly processed as negative/positive or are they just consciously rated so when rated with self-report scales (Armitage et al., 2020; Lahdelma et al., 2024).

### Dataset details

This is a simple experiment. Each participants (_N_ = 40) made decision about word valence in 64 trials (sound primes + word targets). These trials consisted of 8 positive and 8 negative words presented either congruously with the sound (positive sound and positive word or negative sound and negative word) or incongruously (negative sound and positive word or positive sound and negative word). The dependent variable is the reaction time (`reaction_time`) in milliseconds and the two main independent variables are `congruity` (which encodes whether the sound and word are either congruous or incongruous) and musical `expertise` (musician or non-musician). Some previous literature suggest that musicians tend to be more sensitive to these nuances in the sounds.

This is what the data looks like. 
```{r}
#| id: load-data
#| echo: true
#| output: asis
data <- read.csv('data/MPdata_challenge2_rt_dataset.csv')
knitr::kable(head(data))
```
There are `{r} nrow(data)` observations in the data, which is _not exactly correct_ when there has been 40 participants each completing 64 trials (40 x 64 = `{r} 40 * 64`). So the data has errors and noise on top of the fact that reaction times tend to be quite noisy as well.

## Challenge

Was there a statistically significant effect of priming? And what about the expertise, was there an expertise difference in the reaction times? Before you will be able to run the analysis, you need to have a look at the data, and find out any quality issues and decide how to tackle them.

> [!TIP]  
> * `expand` function (from `tidyr`) helps you to diagnose (e.g., `data %>% expand(nesting(congruity, expertise))`). `table` function can also be useful (e.g. `table(data$expertise,data$congruity)`) 
> * Histograms are useful.
> * Reaction time data has a feasible range of values.
> * There are 11 different types of errors and noise in the data.

## Solutions

It is all about learning. Again, submit your solutions as Rmarkdown notebook, preferably containing a section of **1. Dealing with errors** and **2. Analysis results**. In the first part, describe what you do and give a rationale of why you remove/alter any observations. You can include figures in your solution.  

> [!IMPORTANT] 
> If you want to engage with study **preregistrations** in future, you need to specify all these steps (1. what are the steps to data exclusion, 2. what are the preprocessing steps, and 3. what are the specific analysis operations) in advance of the data collection. 


## References

* Armitage, J., Lahdelma, I., Eerola, T., & Ambrazevičius, R. (2023). Culture influences conscious appraisal of, but not automatic aversion to, acoustically rough musical intervals. _Plos One, 18(12)_, e0294645. [https://doi.org/10.1371/journal.pone.0294645](https://doi.org/10.1371/journal.pone.0294645)

* Howell, D. C. (2010). _Statistical methods for psychology_. 7th ed.  Wadwsworth, Cengage Learning. 
* Lahdelma, I. & Eerola, T. (2024). Valenced Priming with Acquired Affective Concepts in Music: Automatic Reactions to Common Tonal Chords. Music Perception, 41(3), 161-175. [https://doi.org/10.1525/mp.2024.41.3.161](https://doi.org/10.1525/mp.2024.41.3.161)

* Lilja, D. J. & Linse, G. M. (2022). _Linear regression using R: An introduction to data modeling._  University of Minnesota Libraries Publishing. https://staging.open.umn.edu/opentextbooks/textbooks/linear-regression-using-r-an-introduction-to-data-modeling
* McNulty, K. (2021). _Handbook of regression modeling in people analytics: With examples in R and Python._  Chapman and Hall/CRC. https://peopleanalytics-regression-book.org/index.html
* Sheather, S. (2009). _A Modern Approach to Regression with R_. Springer Science & Business Media. https://link.springer.com/book/10.1007/978-0-387-09608-7
* Tabachnick, B. G., Fidell, L. S., & Osterlind, S. J. (2013). _Using multivariate statistics_. Pearson, Boston, MA. 
* Zhang, K., Zhang, H., Li, S., Yang, C., & Sun, L. (2018). The PMEmo dataset for music emotion recognition. _Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval_, 135–142. [https://doi.org/10.1145/3206025.3206037](https://doi.org/10.1145/3206025.3206037)

